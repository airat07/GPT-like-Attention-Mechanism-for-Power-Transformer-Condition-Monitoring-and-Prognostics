# GPT-like-Attention-Mechanism-for-Power-Transformer-Condition-Monitoring-and-Prognostics
Description:
This research explores the application of Large-Scale Foundation (LSF) models in power transformer prognostic health management (PHM). Leveraging the capabilities of transformer-based language models, the proposed approach aims to enhance the predictive capabilities of PHM systems for power transformers. By integrating the attention mechanisms of the deep learning transformer architecture with structured power transformer PHM data, the system gains the ability to efficiently analyze diverse data sources, including textual descriptions, sensor data, and discrete sinusoidal waveforms. The advanced natural language processing techniques of the transformer architecture effectively interpret transformer-related data and extract valuable insights for detected fault classification and remaining useful life estimations. This study also evaluates this solution against traditional machine learning techniques such as polynomial regression and support vector classification, showcasing the potential advantages that the transformer architecture provides in improving power transformer condition monitoring and contributing to proactive and efficient maintenance strategies.


Functions:
Remaining Useful Life: Use of degradation-based RUL model to estimate the time until transformer requires maintenance. 

![image](https://github.com/airat07/GPT-like-Attention-Mechanism-for-Power-Transformer-Condition-Monitoring-and-Prognostics/assets/102396976/0e1e3dbf-6e39-45f6-92ae-50fa1f525c5a)

Fault Classifications:
